"""
ãƒã‚¤ã‚¢ã‚¹æ¤œå‡ºãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
æ—¥æœ¬ä¼æ¥­ã®ç«¶äº‰åŠ›åˆ†æã«ãŠã‘ã‚‹ãƒã‚¤ã‚¢ã‚¹ã‚’æ¤œå‡ºãƒ»è©•ä¾¡ã™ã‚‹
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass
from enum import Enum
import re
from textblob import TextBlob
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import warnings

class BiasType(Enum):
    """ãƒã‚¤ã‚¢ã‚¹ã®ç¨®é¡ã‚’å®šç¾©"""
    CONFIRMATION = "confirmation_bias"  # ç¢ºè¨¼ãƒã‚¤ã‚¢ã‚¹
    ANCHORING = "anchoring_bias"       # ã‚¢ãƒ³ã‚«ãƒªãƒ³ã‚°åŠ¹æœ
    AVAILABILITY = "availability_bias"  # åˆ©ç”¨å¯èƒ½æ€§ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯
    RECENCY = "recency_bias"           # è¿‘æ™‚åŠ¹æœ
    SURVIVORSHIP = "survivorship_bias" # ç”Ÿå­˜è€…ãƒã‚¤ã‚¢ã‚¹
    NATIONALITY = "nationality_bias"   # å›½ç±ãƒã‚¤ã‚¢ã‚¹
    SIZE = "size_bias"                 # è¦æ¨¡ãƒã‚¤ã‚¢ã‚¹
    INDUSTRY = "industry_bias"         # æ¥­ç•Œãƒã‚¤ã‚¢ã‚¹

@dataclass
class BiasDetectionResult:
    """ãƒã‚¤ã‚¢ã‚¹æ¤œå‡ºçµæœã‚’æ ¼ç´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    bias_type: BiasType
    severity: float  # 0-1ã®ã‚¹ã‚³ã‚¢
    description: str
    evidence: List[str]
    recommendations: List[str]
    confidence: float  # æ¤œå‡ºã®ä¿¡é ¼åº¦

class BiasDetector:
    """ãƒã‚¤ã‚¢ã‚¹æ¤œå‡ºã®ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.japanese_companies = self._load_japanese_companies()
        self.market_categories = {
            'high_share': ['ãƒ­ãƒœãƒƒãƒˆ', 'å†…è¦–é¡', 'å·¥ä½œæ©Ÿæ¢°', 'é›»å­ææ–™', 'ç²¾å¯†æ¸¬å®šæ©Ÿå™¨'],
            'declining_share': ['è‡ªå‹•è»Š', 'é‰„é‹¼', 'ã‚¹ãƒãƒ¼ãƒˆå®¶é›»', 'ãƒãƒƒãƒ†ãƒªãƒ¼', 'PCãƒ»å‘¨è¾ºæ©Ÿå™¨'],
            'lost_share': ['å®¶é›»', 'åŠå°ä½“', 'ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³', 'PC', 'é€šä¿¡æ©Ÿå™¨']
        }
        
    def _load_japanese_companies(self) -> Dict[str, List[str]]:
        """æ—¥æœ¬ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        return {
            'high_share': [
                'ãƒ•ã‚¡ãƒŠãƒƒã‚¯', 'å®‰å·é›»æ©Ÿ', 'å·å´é‡å·¥æ¥­', 'ä¸äºŒè¶Š', 'ãƒ‡ãƒ³ã‚½ãƒ¼ã‚¦ã‚§ãƒ¼ãƒ–',
                'ã‚ªãƒªãƒ³ãƒ‘ã‚¹', 'HOYA', 'å¯Œå£«ãƒ•ã‚¤ãƒ«ãƒ ', 'ã‚­ãƒ¤ãƒãƒ³ãƒ¡ãƒ‡ã‚£ã‚«ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã‚º',
                'DMGæ£®ç²¾æ©Ÿ', 'ãƒ¤ãƒã‚¶ã‚­ãƒã‚¶ãƒƒã‚¯', 'ã‚ªãƒ¼ã‚¯ãƒ', 'ç‰§é‡ãƒ•ãƒ©ã‚¤ã‚¹è£½ä½œæ‰€',
                'æ‘ç”°è£½ä½œæ‰€', 'TDK', 'äº¬ã‚»ãƒ©', 'å¤ªé™½èª˜é›»', 'ã‚­ãƒ¼ã‚¨ãƒ³ã‚¹', 'å³¶æ´¥è£½ä½œæ‰€'
            ],
            'declining_share': [
                'ãƒˆãƒ¨ã‚¿è‡ªå‹•è»Š', 'æ—¥ç”£è‡ªå‹•è»Š', 'ãƒ›ãƒ³ãƒ€', 'ã‚¹ã‚ºã‚­', 'ãƒãƒ„ãƒ€',
                'æ—¥æœ¬è£½é‰„', 'JFEãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹', 'ç¥æˆ¸è£½é‹¼æ‰€',
                'ãƒ‘ãƒŠã‚½ãƒ‹ãƒƒã‚¯', 'ã‚·ãƒ£ãƒ¼ãƒ—', 'ãƒ‘ãƒŠã‚½ãƒ‹ãƒƒã‚¯ã‚¨ãƒŠã‚¸ãƒ¼'
            ],
            'lost_share': [
                'ã‚½ãƒ‹ãƒ¼', 'æ±èŠ', 'NEC', 'å¯Œå£«é€š', 'æ—¥ç«‹è£½ä½œæ‰€',
                'ãƒ«ãƒã‚µã‚¹ã‚¨ãƒ¬ã‚¯ãƒˆãƒ­ãƒ‹ã‚¯ã‚¹', 'ä¸‰è±é›»æ©Ÿ', 'äº¬ã‚»ãƒ©'
            ]
        }
    
    def detect_all_biases(self, analysis_data: Dict[str, Any]) -> List[BiasDetectionResult]:
        """ã™ã¹ã¦ã®ãƒã‚¤ã‚¢ã‚¹ã‚¿ã‚¤ãƒ—ã‚’æ¤œå‡º"""
        results = []
        
        # å„ãƒã‚¤ã‚¢ã‚¹æ¤œå‡ºãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®Ÿè¡Œ
        results.extend(self._detect_confirmation_bias(analysis_data))
        results.extend(self._detect_anchoring_bias(analysis_data))
        results.extend(self._detect_availability_bias(analysis_data))
        results.extend(self._detect_recency_bias(analysis_data))
        results.extend(self._detect_survivorship_bias(analysis_data))
        results.extend(self._detect_nationality_bias(analysis_data))
        results.extend(self._detect_size_bias(analysis_data))
        results.extend(self._detect_industry_bias(analysis_data))
        
        return sorted(results, key=lambda x: x.severity, reverse=True)
    
    def _detect_confirmation_bias(self, data: Dict[str, Any]) -> List[BiasDetectionResult]:
        """ç¢ºè¨¼ãƒã‚¤ã‚¢ã‚¹ã‚’æ¤œå‡º"""
        results = []
        
        # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®åã‚Šã‚’ãƒã‚§ãƒƒã‚¯
        if 'data_sources' in data:
            japanese_sources = sum(1 for src in data['data_sources'] 
                                    if any(jp in src.lower() for jp in ['japan', 'nippon', 'nikkei']))
            total_sources = len(data['data_sources'])
            
            if total_sources > 0 and japanese_sources / total_sources > 0.7:
                severity = min(japanese_sources / total_sources, 1.0)
                results.append(BiasDetectionResult(
                    bias_type=BiasType.CONFIRMATION,
                    severity=severity,
                    description="æ—¥æœ¬é–¢é€£ã®æƒ…å ±æºã«åé‡ã—ã¦ã„ã‚‹å¯èƒ½æ€§",
                    evidence=[f"æ—¥æœ¬é–¢é€£ã‚½ãƒ¼ã‚¹: {japanese_sources}/{total_sources}"],
                    recommendations=[
                        "å›½éš›çš„ãªæƒ…å ±æºã‚’è¿½åŠ ã™ã‚‹",
                        "ç«¶åˆä»–å›½ã®è¦–ç‚¹ã‚‚å«ã‚ã‚‹",
                        "ä¸­ç«‹çš„ãªç¬¬ä¸‰è€…æ©Ÿé–¢ã®åˆ†æã‚’å‚ç…§ã™ã‚‹"
                    ],
                    confidence=0.8
                ))
        
        # ãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–åˆ¤å®šã®åã‚Šã‚’ãƒã‚§ãƒƒã‚¯
        if 'analysis_text' in data:
            text = data['analysis_text']
            positive_keywords = ['å¼·ã¿', 'å„ªä½', 'æˆåŠŸ', 'æŠ€è¡“åŠ›', 'ã‚·ã‚§ã‚¢æ‹¡å¤§']
            negative_keywords = ['èª²é¡Œ', 'åŠ£å‹¢', 'å¾Œé€€', 'ç«¶äº‰æ¿€åŒ–', 'ã‚·ã‚§ã‚¢ä½ä¸‹']
            
            pos_count = sum(text.count(word) for word in positive_keywords)
            neg_count = sum(text.count(word) for word in negative_keywords)
            
            if pos_count + neg_count > 0:
                pos_ratio = pos_count / (pos_count + neg_count)
                if pos_ratio > 0.8 or pos_ratio < 0.2:
                    severity = abs(pos_ratio - 0.5) * 2
                    results.append(BiasDetectionResult(
                        bias_type=BiasType.CONFIRMATION,
                        severity=severity,
                        description="åˆ†æã®è©•ä¾¡ãŒæ¥µç«¯ã«åã£ã¦ã„ã‚‹",
                        evidence=[f"ãƒã‚¸ãƒ†ã‚£ãƒ–è¡¨ç¾ç‡: {pos_ratio:.1%}"],
                        recommendations=[
                            "ã‚ˆã‚Šä¸­ç«‹çš„ãªè¦–ç‚¹ã§åˆ†æã™ã‚‹",
                            "ãƒ‡ãƒ¡ãƒªãƒƒãƒˆã¨ãƒ¡ãƒªãƒƒãƒˆã‚’å‡ç­‰ã«æ¤œè¨ã™ã‚‹"
                        ],
                        confidence=0.7
                    ))
        
        return results
    
    def _detect_anchoring_bias(self, data: Dict[str, Any]) -> List[BiasDetectionResult]:
        """ã‚¢ãƒ³ã‚«ãƒªãƒ³ã‚°åŠ¹æœã‚’æ¤œå‡º"""
        results = []
        
        if 'historical_data' in data:
            hist_data = data['historical_data']
            
            # éå»ã®æˆåŠŸä½“é¨“ã¸ã®éåº¦ãªä¾å­˜ã‚’ãƒã‚§ãƒƒã‚¯
            past_success_mentions = 0
            recent_performance = 0
            
            for record in hist_data:
                if 'year' in record and record['year'] < 2010:
                    if any(keyword in str(record).lower() 
                            for keyword in ['ä¸–ç•Œä¸€', 'æœ€å¤§æ‰‹', 'åœ§å€’çš„', 'ç‹¬å ']):
                        past_success_mentions += 1
                elif 'year' in record and record['year'] >= 2020:
                    if 'performance_score' in record:
                        recent_performance = record['performance_score']
            
            if past_success_mentions > 3 and recent_performance < 0.6:
                severity = min(past_success_mentions / 5.0, 1.0)
                results.append(BiasDetectionResult(
                    bias_type=BiasType.ANCHORING,
                    severity=severity,
                    description="éå»ã®æˆåŠŸä½“é¨“ã«å›ºåŸ·ã—ã¦ã„ã‚‹å¯èƒ½æ€§",
                    evidence=[f"éå»ã®æˆåŠŸè¨€åŠ: {past_success_mentions}å›"],
                    recommendations=[
                        "ç¾åœ¨ã®ç«¶äº‰ç’°å¢ƒã‚’é‡è¦–ã—ãŸåˆ†æã‚’è¡Œã†",
                        "éå»ã®å‰ææ¡ä»¶ã®å¤‰åŒ–ã‚’è€ƒæ…®ã™ã‚‹"
                    ],
                    confidence=0.75
                ))
        
        return results
    
    def _detect_availability_bias(self, data: Dict[str, Any]) -> List[BiasDetectionResult]:
        """åˆ©ç”¨å¯èƒ½æ€§ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ã‚’æ¤œå‡º"""
        results = []
        
        if 'company_mentions' in data:
            mentions = data['company_mentions']
            
            # æœ‰åä¼æ¥­ã¸ã®éåº¦ãªæ³¨ç›®ã‚’ãƒã‚§ãƒƒã‚¯
            famous_companies = ['ãƒˆãƒ¨ã‚¿', 'ã‚½ãƒ‹ãƒ¼', 'ãƒ‘ãƒŠã‚½ãƒ‹ãƒƒã‚¯', 'ãƒ›ãƒ³ãƒ€', 'æ—¥ç”£']
            total_mentions = sum(mentions.values())
            famous_mentions = sum(mentions.get(company, 0) for company in famous_companies)
            
            if total_mentions > 0 and famous_mentions / total_mentions > 0.6:
                severity = famous_mentions / total_mentions
                results.append(BiasDetectionResult(
                    bias_type=BiasType.AVAILABILITY,
                    severity=severity,
                    description="æœ‰åä¼æ¥­ã«åˆ†æãŒåã£ã¦ã„ã‚‹",
                    evidence=[f"æœ‰åä¼æ¥­è¨€åŠç‡: {famous_mentions/total_mentions:.1%}"],
                    recommendations=[
                        "ä¸­å°ãƒ»ä¸­å …ä¼æ¥­ã‚‚å«ã‚ãŸåŒ…æ‹¬çš„ãªåˆ†æã‚’è¡Œã†",
                        "æ¥­ç•Œå…¨ä½“ã®æ§‹é€ ã‚’æŠŠæ¡ã™ã‚‹"
                    ],
                    confidence=0.8
                ))
        
        return results
    
    def _detect_recency_bias(self, data: Dict[str, Any]) -> List[BiasDetectionResult]:
        """è¿‘æ™‚åŠ¹æœã‚’æ¤œå‡º"""
        results = []
        
        if 'time_weighted_data' in data:
            time_data = data['time_weighted_data']
            recent_weight = sum(w for year, w in time_data.items() if int(year) >= 2022)
            total_weight = sum(time_data.values())
            
            if total_weight > 0 and recent_weight / total_weight > 0.8:
                severity = recent_weight / total_weight
                results.append(BiasDetectionResult(
                    bias_type=BiasType.RECENCY,
                    severity=severity,
                    description="æœ€è¿‘ã®å‡ºæ¥äº‹ã«éåº¦ã«å½±éŸ¿ã•ã‚Œã¦ã„ã‚‹",
                    evidence=[f"ç›´è¿‘ãƒ‡ãƒ¼ã‚¿é‡ã¿: {recent_weight/total_weight:.1%}"],
                    recommendations=[
                        "é•·æœŸçš„ãªãƒˆãƒ¬ãƒ³ãƒ‰ã‚‚è€ƒæ…®ã™ã‚‹",
                        "ä¸€æ™‚çš„ãªå¤‰å‹•ã¨æ§‹é€ çš„å¤‰åŒ–ã‚’åŒºåˆ¥ã™ã‚‹"
                    ],
                    confidence=0.7
                ))
        
        return results
    
    def _detect_survivorship_bias(self, data: Dict[str, Any]) -> List[BiasDetectionResult]:
        """ç”Ÿå­˜è€…ãƒã‚¤ã‚¢ã‚¹ã‚’æ¤œå‡º"""
        results = []
        
        if 'companies_analyzed' in data:
            companies = data['companies_analyzed']
            
            # æ’¤é€€ãƒ»å€’ç”£ä¼æ¥­ã®åˆ†æä¸è¶³ã‚’ãƒã‚§ãƒƒã‚¯
            surviving_companies = [c for c in companies if not any(
                keyword in c.lower() for keyword in ['æ’¤é€€', 'è§£æ•£', 'ç ´ç¶»', 'å£²å´']
            )]
            
            survival_rate = len(surviving_companies) / len(companies) if companies else 0
            
            if survival_rate > 0.9 and len(companies) > 10:
                severity = survival_rate
                results.append(BiasDetectionResult(
                    bias_type=BiasType.SURVIVORSHIP,
                    severity=severity,
                    description="æˆåŠŸä¼æ¥­ã®ã¿ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã‚‹å¯èƒ½æ€§",
                    evidence=[f"ç”Ÿå­˜ä¼æ¥­ç‡: {survival_rate:.1%}"],
                    recommendations=[
                        "å¤±æ•—äº‹ä¾‹ã‹ã‚‰ã‚‚å­¦ç¿’ã‚’è¡Œã†",
                        "æ’¤é€€ä¼æ¥­ã®è¦å› åˆ†æã‚’å«ã‚ã‚‹"
                    ],
                    confidence=0.75
                ))
        
        return results
    
    def _detect_nationality_bias(self, data: Dict[str, Any]) -> List[BiasDetectionResult]:
        """å›½ç±ãƒã‚¤ã‚¢ã‚¹ã‚’æ¤œå‡º"""
        results = []
        
        if 'competitive_analysis' in data:
            analysis = data['competitive_analysis']
            
            # æ—¥æœ¬ä¼æ¥­ã¸ã®è©•ä¾¡ã®ç”˜ã•ã‚’ãƒã‚§ãƒƒã‚¯
            japanese_scores = []
            foreign_scores = []
            
            for company, score in analysis.items():
                if any(jp_company in company for jp_company in self.japanese_companies['high_share'] + 
                        self.japanese_companies['declining_share'] + self.japanese_companies['lost_share']):
                    japanese_scores.append(score)
                else:
                    foreign_scores.append(score)
            
            if japanese_scores and foreign_scores:
                jp_avg = np.mean(japanese_scores)
                foreign_avg = np.mean(foreign_scores)
                
                if jp_avg - foreign_avg > 0.3:  # æ—¥æœ¬ä¼æ¥­ã®è©•ä¾¡ãŒ0.3ä»¥ä¸Šé«˜ã„
                    severity = min((jp_avg - foreign_avg) / 0.5, 1.0)
                    results.append(BiasDetectionResult(
                        bias_type=BiasType.NATIONALITY,
                        severity=severity,
                        description="æ—¥æœ¬ä¼æ¥­ã¸ã®è©•ä¾¡ãŒç”˜ã„å¯èƒ½æ€§",
                        evidence=[f"æ—¥æœ¬ä¼æ¥­å¹³å‡: {jp_avg:.2f}, å¤–å›½ä¼æ¥­å¹³å‡: {foreign_avg:.2f}"],
                        recommendations=[
                            "è©•ä¾¡åŸºæº–ã‚’çµ±ä¸€ã™ã‚‹",
                            "ç¬¬ä¸‰è€…ã«ã‚ˆã‚‹è©•ä¾¡ã‚‚å‚è€ƒã«ã™ã‚‹"
                        ],
                        confidence=0.8
                    ))
        
        return results
    
    def _detect_size_bias(self, data: Dict[str, Any]) -> List[BiasDetectionResult]:
        """è¦æ¨¡ãƒã‚¤ã‚¢ã‚¹ã‚’æ¤œå‡º"""
        results = []
        
        if 'company_sizes' in data:
            sizes = data['company_sizes']
            large_company_ratio = sum(1 for s in sizes.values() if s == 'large') / len(sizes)
            
            if large_company_ratio > 0.8:
                severity = large_company_ratio
                results.append(BiasDetectionResult(
                    bias_type=BiasType.SIZE,
                    severity=severity,
                    description="å¤§ä¼æ¥­ã«åˆ†æãŒåã£ã¦ã„ã‚‹",
                    evidence=[f"å¤§ä¼æ¥­æ¯”ç‡: {large_company_ratio:.1%}"],
                    recommendations=[
                        "ä¸­å°ä¼æ¥­ã®é©æ–°æ€§ã‚‚è©•ä¾¡ã™ã‚‹",
                        "ä¼æ¥­è¦æ¨¡åˆ¥ã®åˆ†æã‚’è¡Œã†"
                    ],
                    confidence=0.7
                ))
        
        return results
    
    def _detect_industry_bias(self, data: Dict[str, Any]) -> List[BiasDetectionResult]:
        """æ¥­ç•Œãƒã‚¤ã‚¢ã‚¹ã‚’æ¤œå‡º"""
        results = []
        
        if 'industry_focus' in data:
            industries = data['industry_focus']
            
            # ç‰¹å®šæ¥­ç•Œã¸ã®éåº¦ãªé›†ä¸­ã‚’ãƒã‚§ãƒƒã‚¯
            max_industry_ratio = max(industries.values()) if industries else 0
            
            if max_industry_ratio > 0.6:
                dominant_industry = max(industries, key=industries.get)
                severity = max_industry_ratio
                results.append(BiasDetectionResult(
                    bias_type=BiasType.INDUSTRY,
                    severity=severity,
                    description=f"{dominant_industry}æ¥­ç•Œã«åˆ†æãŒåã£ã¦ã„ã‚‹",
                    evidence=[f"{dominant_industry}: {max_industry_ratio:.1%}"],
                    recommendations=[
                        "æ¥­ç•Œæ¨ªæ–­çš„ãªåˆ†æã‚’è¡Œã†",
                        "æ¥­ç•Œç‰¹æœ‰ã®è¦å› ã‚’è€ƒæ…®ã™ã‚‹"
                    ],
                    confidence=0.75
                ))
        
        return results
    
    def generate_bias_report(self, bias_results: List[BiasDetectionResult]) -> str:
        """ãƒã‚¤ã‚¢ã‚¹æ¤œå‡ºçµæœã®ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        if not bias_results:
            return "ãƒã‚¤ã‚¢ã‚¹ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        report = "## ãƒã‚¤ã‚¢ã‚¹æ¤œå‡ºãƒ¬ãƒãƒ¼ãƒˆ\n\n"
        
        # é‡è¦åº¦é †ã«ã‚½ãƒ¼ãƒˆ
        high_severity = [r for r in bias_results if r.severity > 0.7]
        medium_severity = [r for r in bias_results if 0.4 <= r.severity <= 0.7]
        low_severity = [r for r in bias_results if r.severity < 0.4]
        
        if high_severity:
            report += "### ğŸš¨ é«˜ãƒªã‚¹ã‚¯ï¼ˆé‡è¦åº¦: é«˜ï¼‰\n"
            for result in high_severity:
                report += f"- **{result.bias_type.value}** (é‡è¦åº¦: {result.severity:.1%})\n"
                report += f"  - {result.description}\n"
                report += f"  - æ¨å¥¨å¯¾ç­–: {', '.join(result.recommendations)}\n\n"
        
        if medium_severity:
            report += "### âš ï¸ ä¸­ãƒªã‚¹ã‚¯ï¼ˆé‡è¦åº¦: ä¸­ï¼‰\n"
            for result in medium_severity:
                report += f"- **{result.bias_type.value}** (é‡è¦åº¦: {result.severity:.1%})\n"
                report += f"  - {result.description}\n\n"
        
        if low_severity:
            report += "### â„¹ï¸ ä½ãƒªã‚¹ã‚¯ï¼ˆé‡è¦åº¦: ä½ï¼‰\n"
            for result in low_severity:
                report += f"- {result.bias_type.value}: {result.description}\n"
        
        # ç·åˆçš„ãªæ¨å¥¨äº‹é …
        report += "\n### ğŸ“‹ ç·åˆçš„ãªæ¨å¥¨äº‹é …\n"
        report += "1. å¤šæ§˜ãªæƒ…å ±æºã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã™ã‚‹\n"
        report += "2. å®šé‡çš„ãªæŒ‡æ¨™ã¨å®šæ€§çš„ãªè©•ä¾¡ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹\n"
        report += "3. å›½éš›çš„ãªè¦–ç‚¹ã‚’å«ã‚ã‚‹\n"
        report += "4. é•·æœŸçš„ãƒˆãƒ¬ãƒ³ãƒ‰ã¨çŸ­æœŸçš„å¤‰å‹•ã‚’åŒºåˆ¥ã™ã‚‹\n"
        report += "5. å¤±æ•—äº‹ä¾‹ã‹ã‚‰ã‚‚å­¦ç¿’ã™ã‚‹\n"
        
        return report
    
    def get_bias_score(self, bias_results: List[BiasDetectionResult]) -> float:
        """ç·åˆçš„ãªãƒã‚¤ã‚¢ã‚¹ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆ0-1ã€0ãŒæœ€è‰¯ï¼‰"""
        if not bias_results:
            return 0.0
        
        # é‡ã¿ä»˜ãå¹³å‡ã‚’è¨ˆç®—
        weighted_scores = []
        for result in bias_results:
            weight = result.confidence * (1.0 if result.severity > 0.7 else 
                                        0.7 if result.severity > 0.4 else 0.4)
            weighted_scores.append(result.severity * weight)
        
        return np.mean(weighted_scores)

# ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
def demo_bias_detection():
    """ãƒã‚¤ã‚¢ã‚¹æ¤œå‡ºã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    
    # ã‚µãƒ³ãƒ—ãƒ«åˆ†æãƒ‡ãƒ¼ã‚¿
    sample_data = {
        'data_sources': [
            'nikkei.com', 'reuters.com', 'japan-times.com', 'bloomberg.jp'
        ],
        'analysis_text': 'æ—¥æœ¬ä¼æ¥­ã®æŠ€è¡“åŠ›ã¯å„ªç§€ã§ã€ä»Šå¾Œã‚‚ç«¶äº‰å„ªä½ã‚’ä¿ã¤ã¨äºˆæƒ³ã•ã‚Œã‚‹ã€‚å¼·ã¿ã‚’æ´»ã‹ã—ãŸæˆ¦ç•¥ãŒé‡è¦ã€‚',
        'company_mentions': {
            'ãƒˆãƒ¨ã‚¿': 15, 'ã‚½ãƒ‹ãƒ¼': 12, 'ãƒ‘ãƒŠã‚½ãƒ‹ãƒƒã‚¯': 8, 
            'ãƒ•ã‚¡ãƒŠãƒƒã‚¯': 3, 'ã‚ªãƒ ãƒ­ãƒ³': 2
        },
        'competitive_analysis': {
            'ãƒˆãƒ¨ã‚¿': 0.85, 'ãƒ†ã‚¹ãƒ©': 0.65, 'ã‚½ãƒ‹ãƒ¼': 0.80, 
            'ã‚µãƒ ã‚¹ãƒ³': 0.60, 'ãƒ•ã‚¡ãƒŠãƒƒã‚¯': 0.90, 'ABB': 0.70
        },
        'companies_analyzed': [
            'ãƒˆãƒ¨ã‚¿', 'ã‚½ãƒ‹ãƒ¼', 'ãƒ‘ãƒŠã‚½ãƒ‹ãƒƒã‚¯', 'ãƒ›ãƒ³ãƒ€', 'æ—¥ç”£',
            'ä¸‰è±é›»æ©Ÿ', 'ã‚ªãƒ ãƒ­ãƒ³', 'ãƒ•ã‚¡ãƒŠãƒƒã‚¯'
        ],
        'time_weighted_data': {
            '2020': 0.1, '2021': 0.15, '2022': 0.3, '2023': 0.35, '2024': 0.1
        },
        'company_sizes': {
            'ãƒˆãƒ¨ã‚¿': 'large', 'ã‚½ãƒ‹ãƒ¼': 'large', 'ãƒ‘ãƒŠã‚½ãƒ‹ãƒƒã‚¯': 'large',
            'ãƒ›ãƒ³ãƒ€': 'large', 'æ—¥ç”£': 'large'
        },
        'industry_focus': {
            'è‡ªå‹•è»Š': 0.4, 'ã‚¨ãƒ¬ã‚¯ãƒˆãƒ­ãƒ‹ã‚¯ã‚¹': 0.35, 'ãƒ­ãƒœãƒƒãƒˆ': 0.15, 'ãã®ä»–': 0.1
        }
    }
    
    detector = BiasDetector()
    results = detector.detect_all_biases(sample_data)
    
    print("=== ãƒã‚¤ã‚¢ã‚¹æ¤œå‡ºçµæœ ===")
    for result in results:
        print(f"{result.bias_type.value}: {result.severity:.2f}")
        print(f"  {result.description}")
        print(f"  æ¨å¥¨: {result.recommendations[0] if result.recommendations else 'ãªã—'}")
        print()
    
    print("=== ãƒ¬ãƒãƒ¼ãƒˆ ===")
    print(detector.generate_bias_report(results))
    print(f"\nç·åˆãƒã‚¤ã‚¢ã‚¹ã‚¹ã‚³ã‚¢: {detector.get_bias_score(results):.2f}")

if __name__ == "__main__":
    demo_bias_detection()